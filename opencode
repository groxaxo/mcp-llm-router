#!/usr/bin/env python3
"""OpenCode CLI - Direct interface to the LLM Router."""

import os
import sys
import json
import httpx
from typing import Optional


def make_llm_request(
    prompt: str,
    model: str = "gpt-4",
    base_url: Optional[str] = None,
    api_key_env: str = "OPENAI_API_KEY",
    max_tokens: int = 2000,
    temperature: float = 0.7
) -> dict:
    """
    Make a direct request to an LLM provider.
    
    Args:
        prompt: The prompt to send
        model: Model identifier
        base_url: Base URL for the API
        api_key_env: Environment variable name for the API key
        max_tokens: Maximum tokens to generate
        temperature: Sampling temperature
    
    Returns:
        Response from the LLM
    """
    # Get API key from environment
    api_key = os.getenv(api_key_env)
    if not api_key:
        return {
            "success": False,
            "error": f"API key not found in environment variable: {api_key_env}"
        }
    
    # Determine base URL
    if base_url is None:
        base_url = "https://api.openai.com/v1"
    
    # Prepare messages
    messages = [{"role": "user", "content": prompt}]
    
    # Make request
    try:
        with httpx.Client(timeout=60.0) as client:
            response = client.post(
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            )
            response.raise_for_status()
            result = response.json()
        
        content = result["choices"][0]["message"]["content"]
        
        return {
            "success": True,
            "content": content,
            "model": model,
            "usage": result.get("usage", {})
        }
    
    except httpx.HTTPStatusError as e:
        return {
            "success": False,
            "error": f"HTTP error {e.response.status_code}: {e.response.text}"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


def main():
    """Main CLI entry point."""
    if len(sys.argv) < 2:
        print("Usage: opencode run \"Your prompt here\"")
        print("       opencode run \"Your prompt\" --model gpt-4 --provider openai")
        print("       opencode run \"Your prompt\" --provider openrouter --model anthropic/claude-3-opus")
        print("       opencode run \"Your prompt\" --provider deepinfra --model meta-llama/Llama-3.1-70B-Instruct")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command != "run":
        print(f"Unknown command: {command}")
        print("Available commands: run")
        sys.exit(1)
    
    if len(sys.argv) < 3:
        print("Error: Please provide a prompt")
        sys.exit(1)
    
    prompt = sys.argv[2]
    
    # Parse optional arguments
    model = "gpt-4"
    provider = "openai"
    
    i = 3
    while i < len(sys.argv):
        if sys.argv[i] == "--model" and i + 1 < len(sys.argv):
            model = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--provider" and i + 1 < len(sys.argv):
            provider = sys.argv[i + 1]
            i += 2
        else:
            i += 1
    
    # Determine provider settings
    if provider == "openai":
        base_url = None
        api_key_env = "OPENAI_API_KEY"
    elif provider == "openrouter":
        base_url = "https://openrouter.ai/api/v1"
        api_key_env = "OPENROUTER_API_KEY"
    elif provider == "deepinfra":
        base_url = "https://api.deepinfra.com/v1/openai"
        api_key_env = "DEEPINFRA_API_KEY"
    else:
        print(f"Unknown provider: {provider}")
        print("Available providers: openai, openrouter, deepinfra")
        sys.exit(1)
    
    print(f"ðŸ¤– Sending request to {provider} ({model})...")
    print()
    
    # Make the request
    result = make_llm_request(
        prompt=prompt,
        model=model,
        base_url=base_url,
        api_key_env=api_key_env
    )
    
    if result["success"]:
        print(result["content"])
        print()
        print(f"âœ“ Token usage: {result['usage']}")
    else:
        print(f"âŒ Error: {result['error']}")
        sys.exit(1)


if __name__ == "__main__":
    main()
